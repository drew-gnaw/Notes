## Synchronization

We're gonna try and make the threads play nice and be productive with each other now!

At its core, we want to ensure `mutual exclusion` of `critial sections`. In other words, only one thread should access `shared` data at a time.

### Communicating in shared data

We can run into issues when threads share a data structure, and they access it simultaneously. Each thread's accesses can be arbitrarily interleaved!

A `critical section` is a chunk of code that access `shared data`.

When two threads access the same shared data, with at least one thread updating said data, this is a `race condition`. The resulting behaviour is unpredictable and a serious bug.

So we employ `Mutual Exclusion`, which is a software mechanism that uses hardware support. It ensures that `critical sections` are executed by one thread at a time.

Two threads can `read` the same shared data at the same time, since there is no unpredictable behaviour. So `read` and `write` must be handled differently.

### Mutual Exclusion

--- 

A `lock` is either held by a thread or available. At most one thread can hold a `lock` at a time.

A thread attempting to acquire a lock that is already held will have to wait for the current thread to give up the `lock`.

Lock operations include `lock` and `unlock`. `lock` tries to acquire the lock, and `unlock` releases the lock currently held by the thread.

Then we can surround accesses to shared data as follows:

```c
lock(&aLock);
// access
unlock(&aLock);
```

So that we never access the same structure simultaneously.

A simple implementation might use a global int that is either 0 is the lock is available, or 1 if a thread is holding it. It would only allow a thread to lock if the lock is 0!

But this introduces a `race condition`. Two threads can think that the lock is available simultaneously, and both lock!

We need to introduce a new `instruction` called an atomic exchange that both reads and writes in the same instruction! But why is this necessary? Can't we just use two separate existing `instructions`? The goal here is to make a single instruction that CANNOT be interrupted. In this way, we ensure that we don't have a weird combination of `reads` and `writes` from two threads.

But even this is not infallible! If we have two `CPU`s, they could still fuck each other up. Thus, we use the `Memory bus` to ensure that one `CPU`'s atomic exchange is not interrupted by another instruction from another `CPU`.

### Spinlocks

Lock where the `waiter` spins, looping on a memory read until lock is acquired.

```asm
        ld $lock, r1
        ld $1, r0
loop:   xchg (r1), r0
        beq r0, held
        br loop
held:
```

We keep swapping 1's until the lock is 0, in which case it is immediately swapped into `r0` in the same instruction where the lock in memory is updated back to 1.

But xchg is expensive AF!! What if we just loop in a normal `ld`, until a 0 is detected. Then we jump to a `try` case, which performs an xchg, which makes sure we aren't grabbing a lock that another thread grabbed 0.0001 ms before us.

Spinlocks waste CPU cycles, like PIO polling. The CPU is waiting for a single thread to progress.

### Blocking lock

The idea is to just block the thread. The thread's `TCB` (Thread Control Block) is stored in the lock's `waiting queue`. Then, whenever the lock becomes available, it unblocks the first thread, which places it in the runnable queue.

Blocking locks can be used for mutual exclusion, but also just `event notification`. We block a thread until the `event` occurs.

### Comparison

Spinlocks are good when we expect short wait times, and minimal contention, since waiting is horrible (the thread continues running).

A blocking lock has MORE overhead, but LESS CPU wasted resources. It's better when the lock may be held for a long time, or is under high contention. But if you think about it, a blocking lock uses some `shared data`: the data inside the lock (`queue`)! So we actually need to protect this part of a blocking lock using `spinlocks` ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­


### Monitors (Mutexes) and condition variables

Mutual exclusion and inter-thread synchronization

A mutex is a `blocking lock` that guarantees `mutual exclusion`. It has the same basic operations `lock` and `unlock`, but they are sometimes called `enter` and `exit`.

A `condition variable` allows threads to synchronize with each other. Each condition variable has its own `waiting queue`.

- wait: blocks until another thread signals
- signal: unblocks a thread currently waiting, if one exists
- broadcast: unblocks all threads currently waiting

A `condition variable` is associated to a `mutex`. 



